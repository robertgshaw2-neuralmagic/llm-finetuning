{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "except:\n",
    "    raise ValueError(\"Set HF_TOKEN enviornment variable equal to your access token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "DTYPE = torch.bfloat16\n",
    "BNB_DTYPE = \"nf4\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_BIAS = \"none\"\n",
    "\n",
    "DATASET_TEXT_FIELD = \"text\"\n",
    "OUTPUT_DIR=\"./training-runs-direct/\"\n",
    "BATCH_SIZE=1\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "LEARNING_RATE = 1.41e-5\n",
    "WEIGHT_DECAY=0.0\n",
    "NUM_TRAIN_EPOCHS=1\n",
    "SEQUENCE_LENGTH=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2597841a238a47b5b8c79c9cc10ffaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/llm-finetuning/env/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_type=DTYPE,\n",
    "    bnb_4bit_quant_type=BNB_DTYPE,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    bias=LORA_BIAS,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=DTYPE,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = min(tokenizer.model_max_length, SEQUENCE_LENGTH)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_seq_len,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "tokenized_dataset_train = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "tokenized_dataset_eval = eval_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    num_proc=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf training-runs-direct\n",
    "%mkdir training-runs-direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset_train, shuffle=True, collate_fn=data_collator, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fadf816b760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"input_layernorm.weight\", \"post_attention_layernorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * NUM_TRAIN_EPOCHS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 / 9846 : inf\n",
      "Step 100 / 9846 : 1.61\n",
      "Step 200 / 9846 : 1.49\n",
      "Step 300 / 9846 : 1.46\n",
      "Step 400 / 9846 : 1.43\n",
      "Step 500 / 9846 : 1.42\n",
      "Step 600 / 9846 : 1.41\n",
      "Step 700 / 9846 : 1.39\n",
      "Step 800 / 9846 : 1.39\n",
      "Step 900 / 9846 : 1.38\n",
      "Step 1000 / 9846 : 1.38\n",
      "Step 1100 / 9846 : 1.37\n",
      "Step 1200 / 9846 : 1.36\n",
      "Step 1300 / 9846 : 1.36\n",
      "Step 1400 / 9846 : 1.36\n",
      "Step 1500 / 9846 : 1.36\n",
      "Step 1600 / 9846 : 1.36\n",
      "Step 1700 / 9846 : 1.36\n",
      "Step 1800 / 9846 : 1.35\n",
      "Step 1900 / 9846 : 1.35\n",
      "Step 2000 / 9846 : 1.34\n",
      "Step 2100 / 9846 : 1.34\n",
      "Step 2200 / 9846 : 1.34\n",
      "Step 2300 / 9846 : 1.34\n",
      "Step 2400 / 9846 : 1.34\n",
      "Step 2500 / 9846 : 1.33\n",
      "Step 2600 / 9846 : 1.33\n",
      "Step 2700 / 9846 : 1.33\n",
      "Step 2800 / 9846 : 1.33\n",
      "Step 2900 / 9846 : 1.33\n",
      "Step 3000 / 9846 : 1.33\n",
      "Step 3100 / 9846 : 1.33\n",
      "Step 3200 / 9846 : 1.33\n",
      "Step 3300 / 9846 : 1.33\n",
      "Step 3400 / 9846 : 1.32\n",
      "Step 3500 / 9846 : 1.32\n",
      "Step 3600 / 9846 : 1.32\n",
      "Step 3700 / 9846 : 1.32\n",
      "Step 3800 / 9846 : 1.32\n",
      "Step 3900 / 9846 : 1.32\n",
      "Step 4000 / 9846 : 1.32\n",
      "Step 4100 / 9846 : 1.32\n",
      "Step 4200 / 9846 : 1.32\n",
      "Step 4300 / 9846 : 1.32\n",
      "Step 4400 / 9846 : 1.32\n",
      "Step 4500 / 9846 : 1.32\n",
      "Step 4600 / 9846 : 1.32\n",
      "Step 4700 / 9846 : 1.32\n",
      "Step 4800 / 9846 : 1.32\n",
      "Step 4900 / 9846 : 1.31\n",
      "Step 5000 / 9846 : 1.31\n",
      "Step 5100 / 9846 : 1.31\n",
      "Step 5200 / 9846 : 1.31\n",
      "Step 5300 / 9846 : 1.31\n",
      "Step 5400 / 9846 : 1.31\n",
      "Step 5500 / 9846 : 1.31\n",
      "Step 5600 / 9846 : 1.31\n",
      "Step 5700 / 9846 : 1.31\n",
      "Step 5800 / 9846 : 1.31\n",
      "Step 5900 / 9846 : 1.31\n",
      "Step 6000 / 9846 : 1.31\n",
      "Step 6100 / 9846 : 1.31\n",
      "Step 6200 / 9846 : 1.31\n",
      "Step 6300 / 9846 : 1.31\n",
      "Step 6400 / 9846 : 1.31\n",
      "Step 6500 / 9846 : 1.31\n",
      "Step 6600 / 9846 : 1.31\n",
      "Step 6700 / 9846 : 1.31\n",
      "Step 6800 / 9846 : 1.31\n",
      "Step 6900 / 9846 : 1.31\n",
      "Step 7000 / 9846 : 1.31\n",
      "Step 7100 / 9846 : 1.31\n",
      "Step 7200 / 9846 : 1.31\n",
      "Step 7300 / 9846 : 1.30\n",
      "Step 7400 / 9846 : 1.30\n",
      "Step 7500 / 9846 : 1.30\n",
      "Step 7600 / 9846 : 1.30\n",
      "Step 7700 / 9846 : 1.30\n",
      "Step 7800 / 9846 : 1.30\n",
      "Step 7900 / 9846 : 1.30\n",
      "Step 8000 / 9846 : 1.30\n",
      "Step 8100 / 9846 : 1.30\n",
      "Step 8200 / 9846 : 1.30\n",
      "Step 8300 / 9846 : 1.30\n",
      "Step 8400 / 9846 : 1.30\n",
      "Step 8500 / 9846 : 1.30\n",
      "Step 8600 / 9846 : 1.30\n",
      "Step 8700 / 9846 : 1.30\n",
      "Step 8800 / 9846 : 1.30\n",
      "Step 8900 / 9846 : 1.30\n",
      "Step 9000 / 9846 : 1.30\n",
      "Step 9100 / 9846 : 1.30\n",
      "Step 9200 / 9846 : 1.30\n",
      "Step 9300 / 9846 : 1.30\n",
      "Step 9400 / 9846 : 1.30\n",
      "Step 9500 / 9846 : 1.30\n",
      "Step 9600 / 9846 : 1.30\n",
      "Step 9700 / 9846 : 1.30\n",
      "Step 9800 / 9846 : 1.30\n"
     ]
    }
   ],
   "source": [
    "EVAL_STEPS = 100\n",
    "\n",
    "for epoch in range(NUM_TRAIN_EPOCHS):\n",
    "    total_loss = 0.\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.detach().float()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % EVAL_STEPS == 0:\n",
    "            print(f\"Step {step} / {len(train_dataloader)} : {(total_loss / step).float() :0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./training-runs-direct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.empty_cache() -> None>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del optimizer\n",
    "del batch \n",
    "del outputs\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    \"### Human: What do you think about ChatGPT?### Assistant:\",\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
